{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "mainfile_copy.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1ph9IqQkTdnlBNGXYzP4LyJMfprzUoP2h",
      "authorship_tag": "ABX9TyP5wJM3aXcEGVRj7mb/BSiz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Vineet2107/EEG_Pipeline/blob/main/mainfile_copy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ElBgLDQJwdKD",
        "outputId": "9a277efe-41fa-4084-e4b1-aea6c2eae7f2"
      },
      "source": [
        "import matplotlib\n",
        "import pathlib\n",
        "\n",
        "import mne\n",
        "print(mne.__version__)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.23.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fUwjmI_Dwgvr",
        "outputId": "577570bb-96a7-48cf-f51b-73b275c96b6e"
      },
      "source": [
        "pip install mne"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting mne\n",
            "  Downloading mne-0.23.4-py3-none-any.whl (6.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.9 MB 9.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from mne) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.7/dist-packages (from mne) (1.19.5)\n",
            "Installing collected packages: mne\n",
            "Successfully installed mne-0.23.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fKNHvOXuxA3s",
        "outputId": "63382cbd-c49c-428f-c6ef-5255398729c9"
      },
      "source": [
        "#The files we will be loading\n",
        "#files = ['10_1', '10_2', '12_1', '12_2', '13_1', '13_2', '14_1', '14_2', '15_1', '15_2', '16_1', '16_2', '21_1', '21_2', '22_1', '22_2', '23_1', '23_2', '28_1', '28_2', '31_1', '31_2', '32_1', '32_2', '33_1', '33_2', '34_1', '34_2', '35_1', '35_2', '46_1', '46_2', '47_1', '47_2', '48_1', '48_2', '49_1', '49_2', '50_1', '50_2', '69_1', '69_2', '70_1', '70_2', '71_1', '71_2', '72_1', '72_2']\n",
        "files = ['10_1', '10_2', '12_1', '12_2', '13_1', '13_2', '28_1', '28_2', '31_1', '31_2', '32_1', '32_2']\n",
        "\n",
        "print(len(files))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v8WXSoJqxJzt"
      },
      "source": [
        "#Paths Arush\n",
        "path = {}\n",
        "for f in files:\n",
        "    path[f] = 'data/other subjects set files/sub-AB{}_eeg_sub-AB{}_task-gonogo_run-{}_eeg.set'.format(f[0:-2], f[0:-2], f[-1])\n",
        "    \n",
        "path_channels_tsv = 'data/other subjects set files/sub-AB10_eeg_sub-AB10_task-gonogo_run-1_channels.tsv'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g1QrIYDWy8oT"
      },
      "source": [
        "#Loading raw data\n",
        "unclean_raw = {}\n",
        "for f in files:\n",
        "    unclean_raw[f] = mne.io.read_raw_eeglab(path[f], eog=(), preload=True, uint16_codec=None, verbose=None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "miMXbeT5y82d"
      },
      "source": [
        "\n",
        "#plotting raw data\n",
        "# rawab6_1.plot();\n",
        "# rawab6_2.plot();\n",
        "unclean_raw['10_1'].plot();\n",
        "# rawab10_2.plot();\n",
        "unclean_raw['12_1'].plot();\n",
        "# rawab12_2.plot();\n",
        "# rawab13_1.plot();\n",
        "# rawab13_2.plot();\n",
        "# rawab28_1.plot();\n",
        "# rawab28_2.plot();\n",
        "# rawab31_1.plot();\n",
        "# rawab31_2.plot();\n",
        "# rawab32_1.plot();\n",
        "# rawab32_2.plot();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sQqZ5REBy85E"
      },
      "source": [
        "#get type of channels from tsv file\n",
        "import pandas as pd\n",
        "channel_types_tsv = pd.read_csv(path_channels_tsv, delimiter = '\\t')\n",
        "\n",
        "def get_channel_types_dic(channel_types_tsv):\n",
        "    channel_types = {}\n",
        "    for i in range(channel_types_tsv.shape[0]):\n",
        "        channel_types[channel_types_tsv['name'][i]] = channel_types_tsv['type'][i].lower()\n",
        "    reassign_dic = {'HEO':'eog', 'VEO':'eog', 'R-Dia-X-(mm)':'misc', 'R-Dia-Y-(mm)':'misc'}\n",
        "    for k in list(reassign_dic.keys()):\n",
        "        if k in list(channel_types):\n",
        "            channel_types[k] = reassign_dic[k]\n",
        "    return channel_types\n",
        "\n",
        "channel_types = get_channel_types_dic(channel_types_tsv)\n",
        "channel_types"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y_FIoJhey87v"
      },
      "source": [
        "#assigning channel types\n",
        "for f in files:\n",
        "    unclean_raw[f].set_channel_types(channel_types)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zBt4z7kgy89w"
      },
      "source": [
        "#filtering the signals\n",
        "for f in files:\n",
        "    unclean_raw[f].filter(0.5, 60)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P4f6gnCmy9CV"
      },
      "source": [
        "unclean_raw['12_1'].plot_sensors(ch_type = 'eeg', sphere = 10);\n",
        "# # rawab12_2.plot_sensors(ch_type = 'eeg', sphere = 10);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XTYNNx3iy9Ey"
      },
      "source": [
        "unclean_raw['12_1'].plot_sensors(ch_type = 'eeg', kind = '3d');\n",
        "# # rawab12_2.plot_sensors(ch_type = 'eeg', kind = '3d');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QMbVQgrly9HZ"
      },
      "source": [
        "unclean_raw['10_1']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1kdMnAAZy9Jn"
      },
      "source": [
        "#Creating copy\n",
        "raw = {}\n",
        "for f in files:\n",
        "    raw[f] = unclean_raw[f].copy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wcdfpIlAy9MM"
      },
      "source": [
        "#Creating ICA objects\n",
        "ica = {}\n",
        "for f in files:\n",
        "    ica[f] = mne.preprocessing.ICA(n_components = 20, random_state = 42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Frz42HIRy9O0"
      },
      "source": [
        "#Fitting ICA objects\n",
        "for f in files:\n",
        "    ica[f].fit(raw[f])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H3daoOSry9RU"
      },
      "source": [
        "#plotting sources and components\n",
        "ica['10_1'].plot_sources(raw['10_1'])\n",
        "ica['10_2'].plot_components(outlines = 'head', sphere = 10, ch_type = 'eeg')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MlKYNqa6y9Ty"
      },
      "source": [
        "#To find and mark bad components\n",
        "def mark_artifacts(ica, raw):\n",
        "    ica.exclude = []\n",
        "    ica.detect_artifacts(raw)\n",
        "    eeg_bads = list(ica.exclude)\n",
        "    ecg_bads = ica.find_bads_ecg(raw)[0]\n",
        "    eog_bads = ica.find_bads_eog(raw)[0]\n",
        "    ica.exclude = list(set(eeg_bads+ecg_bads+eog_bads))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hvoDLzOZy9WR"
      },
      "source": [
        "#Marking bad components\n",
        "for f in files:\n",
        "    mark_artifacts(ica[f], raw[f])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vVTvoxU5y9Z3"
      },
      "source": [
        "ica['10_1'].plot_sources(raw['10_1'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rSTILcJUy9kM"
      },
      "source": [
        "#Applying ica\n",
        "for f in files:\n",
        "    ica[f].apply(raw[f], exclude = ica[f].exclude)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "78CWa72Dy9nA"
      },
      "source": [
        "events = {}\n",
        "for f in files:\n",
        "    events[f] = mne.events_from_annotations(raw[f])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1euXH_2ey9pi"
      },
      "source": [
        "#Creating event id Dictionary\n",
        "event_id = {\n",
        "    \"taskstart\" : '9',\n",
        "    \"cue\" : \"1\",\n",
        "    \"go\" : \"2\",\n",
        "    \"button press\" : \"5\",\n",
        "    \"no-go\" : \"4\",\n",
        "    \"task end\": \"10\",\n",
        "    \"error 1\" : \"3\",\n",
        "    \"error 2\" : \"6\",\n",
        "    \"error 3\" : \"7\",\n",
        "    \"error 4\" : \"8\",\n",
        "    \"error 5\" : \"11\"\n",
        "}\n",
        "event_id"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rf-c9QCKy9tM"
      },
      "source": [
        "#Creating epochs\n",
        "epochs = {}\n",
        "for f in files:\n",
        "    epochs[f] = mne.Epochs(raw[f],\n",
        "                   events = events[f][0],\n",
        "                   event_id = events[f][1],)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gx7vljxq2Had"
      },
      "source": [
        "epochs['10_1'].plot();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pYxhMbBg2Hds"
      },
      "source": [
        "epochs['10_1'].get_data(picks = 'eeg').shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CsfOLLcw2Hgx"
      },
      "source": [
        "#to fix event ids\n",
        "def get_keys_from_value(d, val):\n",
        "    return [k for k, v in d.items() if v == val]\n",
        "def fix_event_ids(epochs, events):\n",
        "    for i in range(epochs.events.shape[0]):\n",
        "        epochs.events[i][2] = int(get_keys_from_value(events[1], epochs.events[i][2])[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "seNpauWA2Hjo"
      },
      "source": [
        "#fix event ids\n",
        "for f in files:\n",
        "    fix_event_ids(epochs[f], events[f])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GKwNpSVO2oVz"
      },
      "source": [
        "#Loading important libraries\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "from tensorflow.keras.layers import Dense, Softmax\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1o_gsSPn2og6"
      },
      "source": [
        "#Getting epoch data\n",
        "datadic = {}\n",
        "for f in files:\n",
        "    datadic[f] = epochs[f].get_data(picks = 'eeg')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KRqvoVHv2oni"
      },
      "source": [
        "#Concatenating data\n",
        "data = np.concatenate(list(datadic.values()), \n",
        "                      axis = 0)\n",
        "print(data.shape)\n",
        "\n",
        "#Changing the shape of data from (events, channel, time points) to (events, time points, channel)\n",
        "datars = np.zeros((data.shape[0], data.shape[2], data.shape[1]))\n",
        "for i in range(datars.shape[0]):\n",
        "    datars[i] = np.transpose(data[i])\n",
        "    \n",
        "dims_lstm_1 = datars.shape[1]\n",
        "dims_lstm_2 = datars.shape[2]\n",
        "print(dims_lstm_1, \"X\", dims_lstm_2)\n",
        "datars.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "udAUGo9T2owU"
      },
      "source": [
        "n_trials = data.shape[0]\n",
        "\n",
        "\n",
        "data = data.reshape(n_trials, -1)\n",
        "print(data.shape)\n",
        "dims_ip = data.shape[1]\n",
        "dims_ip\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eh3DRiBE2ozt"
      },
      "source": [
        "#Getting y\n",
        "ydic = {}\n",
        "for f in files:\n",
        "    ydic[f] = epochs[f].events[:, 2]\n",
        "y = np.concatenate(list(ydic.values()), axis = 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aB-ax8QF2o7p"
      },
      "source": [
        "#Preparing main dataset\n",
        "y = y.reshape(-1, 1)\n",
        "# y = y[...,np.newaxis]\n",
        "datarscopy = datars.copy()\n",
        "datarscopy = datarscopy.reshape(datars.shape[0], -1)\n",
        "\n",
        "print(datars.shape)\n",
        "print(y.shape)\n",
        "main_dataset = np.concatenate((datarscopy, y), axis = 1)\n",
        "print(main_dataset.shape)\n",
        "\n",
        "#Defining main x and y for training\n",
        "main_x = main_dataset[:, :-1]\n",
        "main_y = main_dataset[:, -1]\n",
        "print(main_x.shape)\n",
        "print(main_y.shape)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3cfxmNlN2pJ0"
      },
      "source": [
        "# Clearing memory\n",
        "del unclean_raw\n",
        "del raw\n",
        "del ica\n",
        "del events\n",
        "del epochs\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gvoqxk462Hm9"
      },
      "source": [
        "#Removing events which are not needed for current task\n",
        "dataset = main_dataset.copy()\n",
        "print(dataset.shape)\n",
        "dataset = dataset[dataset[:, -1]!=9]\n",
        "dataset = dataset[dataset[:, -1]!=10]\n",
        "\n",
        "dataset = dataset[dataset[:, -1]!=3]\n",
        "dataset = dataset[dataset[:, -1]!=6]\n",
        "dataset = dataset[dataset[:, -1]!=7]\n",
        "dataset = dataset[dataset[:, -1]!=8]\n",
        "dataset = dataset[dataset[:, -1]!=11]\n",
        "\n",
        "print(dataset.shape)\n",
        "\n",
        "#Defining x and y\n",
        "x = dataset[:, :-1]\n",
        "y = dataset[:, -1]\n",
        "print(x.shape)\n",
        "print(y.shape)\n",
        "print(list(set(y)))\n",
        "\n",
        "permutations = np.random.permutation(len(x))\n",
        "# print(permutation)\n",
        "print(x[-1])\n",
        "x = x[permutations]\n",
        "y = y[permutations]\n",
        "print(x[-1])\n",
        "\n",
        "xmlp = x.copy()\n",
        "ymlp = y.copy()\n",
        "\n",
        "xlstm = x.copy().reshape((-1, dims_lstm_1, dims_lstm_2))\n",
        "ylstm = y.copy()\n",
        "\n",
        "xcnn = x.copy().reshape((-1, dims_lstm_1, dims_lstm_2))\n",
        "ycnn = y.copy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5tbqT0UI3wKC"
      },
      "source": [
        "#Encoding y\n",
        "dict = {1.0: 0,2.0: 1, 4.0: 2, 5.0: 3}\n",
        "\n",
        "for i in range(len(y)):\n",
        "    y[i] = dict[y[i]]\n",
        "    ylstm[i] = dict[ylstm[i]]\n",
        "\n",
        "print(list(set(y)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SfpVsfwK3wXu"
      },
      "source": [
        "num_classes = 4\n",
        "y = tf.keras.utils.to_categorical(y, num_classes)\n",
        "ymlp = y.copy()\n",
        "ylstm = y.copy()\n",
        "ycnn = y.copy()\n",
        "# yab10_1[0]\n",
        "print(y[:5])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pf76k7AD3wau"
      },
      "source": [
        "train_test_split = int(0.8 * len(xmlp))\n",
        "print(train_test_split)\n",
        "\n",
        "x_train_mlp = xmlp[:train_test_split]\n",
        "y_train_mlp = ymlp[:train_test_split]\n",
        "\n",
        "x_test_mlp = xmlp[train_test_split:]\n",
        "y_test_mlp = ymlp[train_test_split:]\n",
        "\n",
        "# x_mlp_train = xmlp[:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TW_RPkt13weP"
      },
      "source": [
        "def get_cyclic_generator(features, labels, batch_size = 1):\n",
        "  while True:\n",
        "    for n in range(features.shape[0]//batch_size):\n",
        "      yield (features[n*batch_size: (n+1)*batch_size], labels[n*batch_size: (n+1)*batch_size])\n",
        "    permuted = np.random.permutation(len(features))\n",
        "    features = features[permuted]\n",
        "    labels = labels[permuted]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VFuBut2M3whk"
      },
      "source": [
        "train_batch_size = 32\n",
        "train_cyclic_generator_mlp = get_cyclic_generator(x_train_mlp, y_train_mlp, batch_size = train_batch_size)\n",
        "# test_cyclic_generator_mlp = get_cyclic_generator(x_test_mlp, y_test_mlp, batch_size = train_batch_size)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h8lflJHa3wkr"
      },
      "source": [
        "#Creating model architecture\n",
        "inp_shape = (dims_ip, )\n",
        "ip = tf.keras.Input(shape = inp_shape)\n",
        "dense1 = tf.keras.layers.Dense(units = 32, activation = 'relu', kernel_initializer = 'random_normal')(ip)\n",
        "drop = tf.keras.layers.Dropout(.4)(dense1)\n",
        "dense2 = tf.keras.layers.Dense(units = 16, activation = 'relu', kernel_initializer = 'random_normal')(drop)\n",
        "drop2 = tf.keras.layers.Dropout(.4)(dense2)\n",
        "out = tf.keras.layers.Dense(num_classes, activation = 'softmax')(drop2)\n",
        "\n",
        "mlp = tf.keras.Model(inputs = ip, outputs = out)\n",
        "mlp.summary()\n",
        "# inp_shape = (None, )\n",
        "# review_sequence = tf.keras.Input(shape = inp_shape)\n",
        "# embedding_sequence = tf.keras.layers.Embedding(input_dim=100+1, output_dim=32, input_shape=(review_sequence.shape), mask_zero=False)(review_sequence)\n",
        "# average_embedding = tf.keras.layers.GlobalAveragePooling1D()(embedding_sequence)\n",
        "# positive_probability = tf.keras.layers.Dense(units=1, activation='sigmoid')(average_embedding)\n",
        "\n",
        "# model = tf.keras.Model(inputs = review_sequence, outputs = positive_probability)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LeaiGTVT3wnf"
      },
      "source": [
        "#Creating model architecture\n",
        "inp_shape = (dims_ip, )\n",
        "ip = tf.keras.Input(shape = inp_shape)\n",
        "dense1 = tf.keras.layers.Dense(units = 32, activation = 'relu', kernel_initializer = 'random_normal')(ip)\n",
        "drop = tf.keras.layers.Dropout(.4)(dense1)\n",
        "dense2 = tf.keras.layers.Dense(units = 16, activation = 'relu', kernel_initializer = 'random_normal')(drop)\n",
        "drop2 = tf.keras.layers.Dropout(.4)(dense2)\n",
        "out = tf.keras.layers.Dense(num_classes, activation = 'softmax')(drop2)\n",
        "\n",
        "mlp = tf.keras.Model(inputs = ip, outputs = out)\n",
        "mlp.summary()\n",
        "# inp_shape = (None, )\n",
        "# review_sequence = tf.keras.Input(shape = inp_shape)\n",
        "# embedding_sequence = tf.keras.layers.Embedding(input_dim=100+1, output_dim=32, input_shape=(review_sequence.shape), mask_zero=False)(review_sequence)\n",
        "# average_embedding = tf.keras.layers.GlobalAveragePooling1D()(embedding_sequence)\n",
        "# positive_probability = tf.keras.layers.Dense(units=1, activation='sigmoid')(average_embedding)\n",
        "\n",
        "# model = tf.keras.Model(inputs = review_sequence, outputs = positive_probability)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z6_I-d9T3wrB"
      },
      "source": [
        "mlp_history = mlp.fit(train_cyclic_generator_mlp,\n",
        "#                         validation_data = test_cyclic_generator_mlp,\n",
        "                      validation_data = (x_test_mlp, y_test_mlp),\n",
        "                      steps_per_epoch= 449,\n",
        "                      epochs = 50,\n",
        "                      callbacks = [mlp_earlystop, mlp_checkpoint]) #Fitting the model\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PuBGg35w5qcq"
      },
      "source": [
        "#Plotting model history\n",
        "plt.plot(mlp_history.history['acc'])\n",
        "plt.plot(mlp_history.history['loss'])\n",
        "plt.legend(['Training Accuracy', 'Training Loss'])\n",
        "plt.title('Training History')\n",
        "plt.ylabel('Value')\n",
        "plt.xlabel('epochs')\n",
        "plt.show()\n",
        "print(\"Initial training accuracy is: \", mlp_history.history['acc'][0])\n",
        "print(\"Final training accuracy is: \", mlp_history.history['acc'][-1])\n",
        "\n",
        "print(\"Initial training loss is: \", mlp_history.history['loss'][0])\n",
        "print(\"Final training loss is: \", mlp_history.history['loss'][-1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nJ8q-4Iw5qoM"
      },
      "source": [
        "#Plotting model history\n",
        "plt.plot(mlp_history.history['val_acc'])\n",
        "plt.plot(mlp_history.history['val_loss'])\n",
        "plt.legend(['Valdation Accuracy', 'Validation Loss'])\n",
        "plt.title('Validation History')\n",
        "plt.ylabel('Value')\n",
        "plt.xlabel('epochs')\n",
        "plt.show()\n",
        "print(\"Initial validation accuracy is: \", mlp_history.history['val_acc'][0])\n",
        "print(\"Final validation accuracy is: \", mlp_history.history['val_acc'][-1])\n",
        "\n",
        "print(\"Initial validation loss is: \", mlp_history.history['val_loss'][0])\n",
        "print(\"Final validation loss is: \", mlp_history.history['val_loss'][-1])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EKmdok2o5qx_"
      },
      "source": [
        "train_test_split = int(0.8 * len(xlstm))\n",
        "print(train_test_split)\n",
        "\n",
        "x_train_lstm = xlstm[:train_test_split]\n",
        "y_train_lstm = ylstm[:train_test_split]\n",
        "\n",
        "x_test_lstm = xlstm[train_test_split:]\n",
        "y_test_lstm = ylstm[train_test_split:]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "haX6uP5d5q7a"
      },
      "source": [
        "def get_cyclic_generator(features, labels, batch_size = 1):\n",
        "  while True:\n",
        "    for n in range(features.shape[0]//batch_size):\n",
        "      yield (features[n*batch_size: (n+1)*batch_size], labels[n*batch_size: (n+1)*batch_size])\n",
        "    permuted = np.random.permutation(len(features))\n",
        "    features = features[permuted]\n",
        "    labels = labels[permuted]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vWpKX-vm5rD-"
      },
      "source": [
        "train_batch_size = 32\n",
        "train_cyclic_generator_lstm = get_cyclic_generator(x_train_lstm, y_train_lstm, batch_size = train_batch_size)\n",
        "# test_cyclic_generator = get_cyclic_generator(x_test, y_test, batch_size = train_batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bOO491Ht5rNx"
      },
      "source": [
        "#Creating model architecture\n",
        "inp_shape = (dims_lstm_1, dims_lstm_2)\n",
        "ip = tf.keras.Input(shape = inp_shape)\n",
        "lstm = tf.keras.layers.LSTM(32, return_sequences=True)(ip)\n",
        "# drop = tf.keras.layers.Dropout(.4)(lstm)\n",
        "# lstm = tf.keras.layers.LSTM(16, return_sequences=True)(ip)\n",
        "# drop = tf.keras.layers.Dropout(.4)(lstm)\n",
        "flatten = tf.keras.layers.Flatten()(lstm)\n",
        "dense1 = tf.keras.layers.Dense(units = 32, kernel_initializer = 'random_normal')(flatten)\n",
        "leakyRelu = tf.keras.layers.LeakyReLU()(dense1)\n",
        "drop1 = tf.keras.layers.Dropout(.4)(leakyRelu)\n",
        "dense2 = tf.keras.layers.Dense(units = 16, kernel_initializer = 'random_normal')(drop1)\n",
        "leakyRelu2 = tf.keras.layers.LeakyReLU()(dense2)\n",
        "drop2 = tf.keras.layers.Dropout(.4)(dense2)\n",
        "out = tf.keras.layers.Dense(num_classes, activation = 'softmax')(drop2)\n",
        "\n",
        "lstm = tf.keras.Model(inputs = ip, outputs = out)\n",
        "lstm.summary()\n",
        "# inp_shape = (None, )\n",
        "# review_sequence = tf.keras.Input(shape = inp_shape)\n",
        "# embedding_sequence = tf.keras.layers.Embedding(input_dim=100+1, output_dim=32, input_shape=(review_sequence.shape), mask_zero=False)(review_sequence)\n",
        "# average_embedding = tf.keras.layers.GlobalAveragePooling1D()(embedding_sequence)\n",
        "# positive_probability = tf.keras.layers.Dense(units=1, activation='sigmoid')(average_embedding)\n",
        "\n",
        "# model = tf.keras.Model(inputs = review_sequence, outputs = positive_probability)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_bzyckF-5rVa"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m6qc_rqK5rcM"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}